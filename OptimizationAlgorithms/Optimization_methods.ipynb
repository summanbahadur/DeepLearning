{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name : Summan Bahadur\n",
    "# Roll Number : MSDSF21M509\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from testCases import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    " \n",
    "    for l in range(L):      \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads['db' + str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "     \n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "    \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "     \n",
    "        mini_batch_X = shuffled_X[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "        mini_batch_Y = shuffled_Y[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "       \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_velocity\n",
    "\n",
    "def initialize_velocity(parameters):\n",
    "       \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "       \n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
    "     \n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_velocity_test_case()\n",
    "\n",
    "v = initialize_velocity(parameters)\n",
    "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_momentum\n",
    "\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "   \n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        \n",
    "        ### START CODE HERE ### (approx. 4 lines)\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + (1 - beta)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + (1 - beta)*grads['db' + str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v[\"db\" + str(l+1)]\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
    "\n",
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
    "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_adam\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "  \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "    ### START CODE HERE ### (approx. 4 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_adam_test_case()\n",
    "\n",
    "v, s = initialize_adam(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_adam\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)] + (1 - beta1)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)] + (1 - beta1)*grads['db' + str(l+1)]\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1 - beta1**t)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1 - beta2)*np.square(grads['dW' + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1 - beta2)*np.square(grads['db' + str(l+1)])\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1 - beta2**t)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
    "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
    "\n",
    "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let's implement a model with each of these optimizers and observe the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    print(\"\\nThe number of training examples is : %i\\n\" %m)\n",
    "    print(\"The mini-batch size : %i\\n\" %mini_batch_size)\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "            print(\"\\n OK OK \\n\")\n",
    "            print(minibatch_Y)\n",
    "            print(\"\\n OK OK \\n\")\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now run this 3 layer neural network with each of the 3 optimization methods.\n",
    "\n",
    "### 5.1 - Mini-batch Gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Mini-batch gradient descent with momentum\n",
    "\n",
    "Run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Momentum optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Mini-batch with Adam mode\n",
    "\n",
    "Run the following code to see how the model does with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "Ckiv2",
   "launcher_item_id": "eNLYh"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "132.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "905ed832d2f4288efef86a21e0f4658ce21fbf9a00a690c2ed28e662bd4105c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
